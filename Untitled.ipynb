{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "acd00bc5-bb17-4423-8382-9b21558a090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "  'good': True,\n",
    "  'bad': False,\n",
    "  'happy': True,\n",
    "  'sad': False,\n",
    "  'not good': False,\n",
    "  'not bad': True,\n",
    "  'not happy': False,\n",
    "  'not sad': True,\n",
    "  'very good': True,\n",
    "  'very bad': False,\n",
    "  'very happy': True,\n",
    "  'very sad': False,\n",
    "  'i am happy': True,\n",
    "  'this is good': True,\n",
    "  'i am bad': False,\n",
    "  'this is bad': False,\n",
    "  'i am sad': False,\n",
    "  'this is sad': False,\n",
    "  'i am not happy': False,\n",
    "  'this is not good': False,\n",
    "  'i am not bad': True,\n",
    "  'this is not sad': True,\n",
    "  'i am very happy': True,\n",
    "  'this is very good': True,\n",
    "  'i am very bad': False,\n",
    "  'this is very sad': False,\n",
    "  'this is very happy': True,\n",
    "  'i am good not bad': True,\n",
    "  'this is good not bad': True,\n",
    "  'i am bad not good': False,\n",
    "  'i am good and happy': True,\n",
    "  'this is not good and not happy': False,\n",
    "  'i am not at all good': False,\n",
    "  'i am not at all bad': True,\n",
    "  'i am not at all happy': False,\n",
    "  'this is not at all sad': True,\n",
    "  'this is not at all happy': False,\n",
    "  'i am good right now': True,\n",
    "  'i am bad right now': False,\n",
    "  'this is bad right now': False,\n",
    "  'i am sad right now': False,\n",
    "  'i was good earlier': True,\n",
    "  'i was happy earlier': True,\n",
    "  'i was bad earlier': False,\n",
    "  'i was sad earlier': False,\n",
    "  'i am very bad right now': False,\n",
    "  'this is very good right now': True,\n",
    "  'this is very sad right now': False,\n",
    "  'this was bad earlier': False,\n",
    "  'this was very good earlier': True,\n",
    "  'this was very bad earlier': False,\n",
    "  'this was very happy earlier': True,\n",
    "  'this was very sad earlier': False,\n",
    "  'i was good and not bad earlier': True,\n",
    "  'i was not good and not happy earlier': False,\n",
    "  'i am not at all bad or sad right now': True,\n",
    "  'i am not at all good or happy right now': False,\n",
    "  'this was not happy and not good earlier': False,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "  'this is happy': True,\n",
    "  'i am good': True,\n",
    "  'this is not happy': False,\n",
    "  'i am not good': False,\n",
    "  'this is not bad': True,\n",
    "  'i am not sad': True,\n",
    "  'i am very good': True,\n",
    "  'this is very bad': False,\n",
    "  'i am very sad': False,\n",
    "  'this is bad not good': False,\n",
    "  'this is good and happy': True,\n",
    "  'i am not good and not happy': False,\n",
    "  'i am not at all sad': True,\n",
    "  'this is not at all good': False,\n",
    "  'this is not at all bad': True,\n",
    "  'this is good right now': True,\n",
    "  'this is sad right now': False,\n",
    "  'this is very bad right now': False,\n",
    "  'this was good earlier': True,\n",
    "  'i was not happy and not good earlier': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "0192ae1c-41fd-4bc7-884c-22e153cdb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary\n",
    "vocab=list(set([w for text in train_data.keys() for w in text.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "abad8db2-86c8-4d5a-9d83-8a3ec32aba39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'was',\n",
       " 'now',\n",
       " 'or',\n",
       " 'right',\n",
       " 'good',\n",
       " 'happy',\n",
       " 'earlier',\n",
       " 'am',\n",
       " 'not',\n",
       " 'this',\n",
       " 'sad',\n",
       " 'and',\n",
       " 'is',\n",
       " 'all',\n",
       " 'bad',\n",
       " 'i',\n",
       " 'very']"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "a284b0f5-58e4-40a5-b892-40d70ae4b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "a88ca8cb-480d-41cc-81de-2ae4acfec4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 unique words found\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(vocab)} unique words found') # 18 unique words found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "4e886421-b7f8-438f-b1c4-d3960c61f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now assigning index to each word by using dictionary\n",
    "word_to_ind={w:i for i,w in enumerate(vocab)}\n",
    "ind_to_word={i:w for i,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "0cc814de-cb6c-485f-a65c-58bf13b220b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ind['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "fbb7043d-5171-4e9a-959c-41e11c24be8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sad'"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_word[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "db86cb67-79bd-4e00-b8e9-6454fa45fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "7c8cdddd-b6b2-4011-aaf7-3742566758aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(text):\n",
    "    one_hot=[]\n",
    "    for w in text.split(' '):\n",
    "        v=np.zeros((len(vocab),1))\n",
    "        v[word_to_ind[w]]=1\n",
    "        one_hot.append(v)\n",
    "    return one_hot\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "2595e9a9-1629-4d28-bc30-baaf4bf570e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now , let's create our recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "2b20ee7c-ba59-4bed-bfb9-e5c117ef600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will be a simple vanilla recurrent neural network which consist only 3 wts and 2 biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "3189e197-a95e-463f-9524-08831665fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self,input_size,output_size,hidden_size=64):\n",
    "        self.Whh=randn(hidden_size,hidden_size)/1000\n",
    "        self.Wxh=randn(hidden_size,input_size)/1000\n",
    "        self.Why=randn(output_size,hidden_size)/1000\n",
    "        self.bh=np.zeros((hidden_size,1))\n",
    "        self.by=np.zeros((output_size,1))\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        h=np.zeros((self.Whh.shape[0],1))\n",
    "\n",
    "        self.last_inputs=inputs\n",
    "        self.last_hs={0:h}\n",
    "        for i,x in enumerate(inputs):\n",
    "            h=np.tanh(np.dot(self.Whh,h)+np.dot(self.Wxh,x)+self.bh)\n",
    "            self.last_hs[i+1]=h\n",
    "        y=np.dot(self.Why,h)+self.by\n",
    "        return y,h\n",
    "\n",
    "    def backward(self,dL_dy,alpha=0.001):  # alpha is learning rate\n",
    "        # first computing gradients for output's wts and biases\n",
    "        dL_dby=dL_dy\n",
    "        n=len(self.last_inputs)\n",
    "        dL_dWhy=np.dot(dL_dy,self.last_hs[n].T)\n",
    "        \n",
    "        # loss gradient wrt last hidden timestep(h)\n",
    "        dL_dh=np.dot(self.Why.T,dL_dy)\n",
    "\n",
    "        # now backpropogating wrt timesteps\n",
    "        for t in reversed(range(n)):\n",
    "            temp=(1-(self.last_hs[t+1])**2)*(dL_dh)\n",
    "            dL_dbh=temp\n",
    "            dL_dWxh=np.dot(temp,self.last_inputs[t].T)\n",
    "            dL_dWhh=np.dot(temp,self.last_hs[t].T)\n",
    "            dL_dh=np.dot(self.Whh,temp)\n",
    "\n",
    "        # we may clip the gradients to prevent them from exploding\n",
    "        for d in [dL_dWhy,dL_dby,dL_dWxh,dL_dWhh,dL_dbh]:\n",
    "            np.clip(d,-1,1,out=d)\n",
    "\n",
    "        # now updating parametrs \n",
    "        self.Why=self.Why-alpha*dL_dWhy\n",
    "        self.by=self.by-alpha*dL_dby\n",
    "        self.Wxh=self.Wxh-alpha*dL_dWxh\n",
    "        self.Whh=self.Whh-alpha*dL_dWhh\n",
    "        self.bh=self.bh-alpha*dL_dbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "621e61ca-d87c-4f19-96df-fe349f5f87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    " def softmax(xs):\n",
    "        xs =xs- np.max(xs, axis=0, keepdims=True)\n",
    "        exp_xs = np.exp(xs)  # Numerically stable\n",
    "        return exp_xs / np.sum(exp_xs, axis=0, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "4d2f65f3-2128-4fa0-82da-43dd9201a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=RNN(vocab_size,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "607af4d6-d6a5-4187-a697-1d5de21efc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=one_hot_encode('i am very good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "67dae142-5641-48bf-a100-17d5cacee1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h,y=rnn.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "5815e9d0-7da0-4de3-b419-acb9fc539b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "565b56d4-8d2d-460f-8ce4-b30e3fabcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "c19fab69-fbde-4ea5-a842-a2c7b2616a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's update our forward to store the provided input and all the h's that it will produce  while propogating forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "4e1e8dc7-74f2-4d2e-8246-303ed823f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's steup the thing foe backward pass in rnn, we will iterate example by example by using wts and biases of previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "b7a78525-ac6d-4458-af25-09a982d0973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in train_data.items():\n",
    "#     inputs=one_hot_encode(inputs)\n",
    "#     target=int(y)\n",
    "\n",
    "#     out,_=rnn.forward(inputs)\n",
    "#     pred=softmax(out)\n",
    "\n",
    "#     # computing dl/dy =p-1 correct label otherwise p\n",
    "#     dL_dy=pred\n",
    "#     dL_dy[target]=dL_dy[target]-1\n",
    "\n",
    "#     rnn.backward(dL_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "15458a38-6aab-41b5-b92e-f0c43a4d6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=train_data.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "b9dd8507-0df1-4a43-9d81-d47f396fbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "1b2b454a-6e49-4bad-afe9-a1e64a057440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items=list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "c7ecc03d-de67-4fd3-ae5e-8f6bf7006187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "f98c855f-4491-47c2-aad7-3abbe6108b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def process_data(data,backprop=True):\n",
    "    items=list(data.items())\n",
    "\n",
    "    random.shuffle(items)\n",
    "    \n",
    "    loss=0\n",
    "    num_correct=0\n",
    "\n",
    "    for x,y in items:\n",
    "        inputs=one_hot_encode(x)\n",
    "        target=int(y)\n",
    "        \n",
    "\n",
    "        out,_y=rnn.forward(inputs)\n",
    "        preds=softmax(out)\n",
    "\n",
    "        # # Check for valid probabilities\n",
    "        # if np.any(preds <= 0):\n",
    "        #     print(\"Invalid preds:\", preds)\n",
    "        #     continue  # Skip or handle error\n",
    "\n",
    "        if (backprop):\n",
    "            \n",
    "            dL_dy=preds.copy()\n",
    "            dL_dy[target]=dL_dy[target]-1\n",
    "            \n",
    "            loss=loss-np.log(preds[target,0]+1e-9)\n",
    "            num_correct=num_correct+int(np.argmax(preds)==target)\n",
    "\n",
    "        \n",
    "            rnn.backward(dL_dy)\n",
    "\n",
    "    return loss/len(items),num_correct/len(items)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "5743fcc3-2914-472a-8037-51de1890df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss,accuracy=process_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "ddc4916a-d547-45ae-9d10-0a8a6a1b255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Epoch 100\n",
      "train_loss=0.6880021520832407,train_accuracy=0.5517241379310345\n",
      "----Epoch 200\n",
      "train_loss=0.6879917621481879,train_accuracy=0.5517241379310345\n",
      "----Epoch 300\n",
      "train_loss=0.6879784906219545,train_accuracy=0.5517241379310345\n",
      "----Epoch 400\n",
      "train_loss=0.6879628970733791,train_accuracy=0.5517241379310345\n",
      "----Epoch 500\n",
      "train_loss=0.6879447510450424,train_accuracy=0.5517241379310345\n",
      "----Epoch 600\n",
      "train_loss=0.6879177122114997,train_accuracy=0.5517241379310345\n",
      "----Epoch 700\n",
      "train_loss=0.6878852672344821,train_accuracy=0.5517241379310345\n",
      "----Epoch 800\n",
      "train_loss=0.6878416521255669,train_accuracy=0.5517241379310345\n",
      "----Epoch 900\n",
      "train_loss=0.6877821251251153,train_accuracy=0.5517241379310345\n",
      "----Epoch 1000\n",
      "train_loss=0.6877182393260775,train_accuracy=0.5517241379310345\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    train_loss,train_accuracy=process_data(train_data)\n",
    "\n",
    "    if (epoch%100)==99:\n",
    "        print(f'----Epoch {epoch+1}')\n",
    "        print(f'train_loss={train_loss},train_accuracy={train_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "618e9e36-2036-4191-8dcc-1d446a9c4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_accuracy=process_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "6e3f1434-a120-456f-b1ba-1873a39e9e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss=0.699326798244231        test_accuracy=50.0%\n"
     ]
    }
   ],
   "source": [
    " print(f'test_loss={test_loss}        test_accuracy={test_accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c94a13-c35f-4358-9e3d-6b8cbc448f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd249e-49ca-4e63-af16-c14f7a0970a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e37d4c-37be-467d-a3bb-4834cbb103ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f948be0-4864-4c4b-aae8-67b900781eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
